<!-- options.html -->
<div class="option-group">
    <h3>Advanced LLM Settings</h3>

    <div class="option">
        <label for="maxChunkSize">Maximum chunk size (characters):</label>
        <input type="number" id="maxChunkSize" min="500" max="8000" step="100" value="2000">
        <div class="option-description">
            <p>Sets the maximum size of text chunks sent to Ollama.</p>
            <p class="hint">Lower values are faster but may reduce quality. Higher values may improve quality but
                require more resources.</p>
        </div>
    </div>

    <div class="option">
        <button id="testOllama" class="secondary-button">Test Ollama Connection</button>
        <div id="ollamaStatus" class="status-message"></div>
        <div class="option-description">
            <p>Test if your Ollama setup is working correctly.</p>
        </div>
    </div>

    <div class="setting-item">
        <label for="maxChunkSize">Maximum Chunk Size:</label>
        <input type="range" id="maxChunkSize" min="2000" max="16000" step="1000" value="8000">
        <span id="maxChunkSizeValue">8000</span> characters
        <p class="description">Larger chunks provide better context but take longer to process</p>
    </div>

    <div class="setting-item">
        <label for="timeout">LLM Request Timeout:</label>
        <input type="range" id="timeout" min="30" max="300" step="30" value="120">
        <span id="timeoutValue">120</span> seconds
        <p class="description">Maximum time to wait for LLM responses</p>
    </div>

    <label>
        Ollama model name:
        <input id="modelName" type="text" placeholder="qwen3:8b" />
    </label>
    <button id="save">Save</button>

</div>
<script src="options.js"></script>